{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tensorflow.core.example import example_pb2\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "def csv_to_tfrecord(schema, csv_file, tfrecorde_file):\n",
    "    reader = csv.DictReader(open(csv_file, 'r'))\n",
    "    examples = []\n",
    "    \n",
    "    for line in reader:\n",
    "        example = example_pb2.Example()\n",
    "        for feature in schema.feature:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "tfrecorde_file = tf.data.TFRecoedDataset(TFRECORD_DAY1)\n",
    "feature_spec = schema_utils.schema_as_feature_spec(SCHEMA).feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://storage.googleapis.com/mlep-public/course_3/week4/C3_W4_Lab_1_starter_files.tar.gz'\n",
    "TAR_NAME = 'C3_W4_Lab_1_starter_files.tar.gz'\n",
    "BASE_DIR = 'starter_files'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "CSV_DIR = os.path.join(DATA_DIR, 'csv')\n",
    "TFRECORD_DIR = os.path.join(DATA_DIR, 'tfrecord')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
    "SCHEMA_FILE = os.path.join(BASE_DIR, 'schema.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import census_constants\n",
    "\n",
    "# Unpack the contents of the constants module\n",
    "_NUMERIC_FEATURE_KEYS = census_constants.NUMERIC_FEATURE_KEYS\n",
    "_VOCAB_FEATURE_DICT = census_constants.VOCAB_FEATURE_DICT\n",
    "_BUCKET_FEATURE_DICT = census_constants.BUCKET_FEATURE_DICT\n",
    "_NUM_OOV_BUCKETS = census_constants.NUM_OOV_BUCKETS\n",
    "_LABEL_KEY = census_constants.LABEL_KEY\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    outputs = {}\n",
    "    for key in _NUMERIC_FEATURE_KEYS:\n",
    "        scaled = tft.scale_to_0_1(inputs[key])\n",
    "        outputs[key] = tf.reshape(scaled, -1)\n",
    "\n",
    "    for key, vocab_size in _VOCAB_FEATURE_DICT.items():\n",
    "        indices = tft.compute_and_apply_vocabulary(inputs[key], num_ov_buckets=_NUM_OOV_BUCKETS)\n",
    "        one_hot = tf.one_hot(indices, vocab_size + _NUM_OOV_BUCKETS)\n",
    "        outputs[key] = tf.reshape(one_hot, [-1, vocab_size + _NUM_OOV_BUCKETS])\n",
    "\n",
    "    for key, num_buckets in _BUCKET_FEATURE_DICT.items():\n",
    "        indices = tft.bucketize(inputs[key], num_buckets)\n",
    "        one_hot = tf.one_hot(indices, num_buckets)\n",
    "        outputs[key] = tf.reshape(one_hot, [-1, num_buckets])\n",
    "    outputs[_LABEL_KEY] = tf.cast(inputs[_LABEL_KEY], tf.float32)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Text\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx.components.trainer.fn_args_utils import DataAccessor, FnArgs\n",
    "from tfx_bsl.public.tfxio import TensorFlowDatasetOptions\n",
    "\n",
    "# import same constants from transform module\n",
    "import census_constants\n",
    "\n",
    "# Unpack the contents of the constants module\n",
    "_NUMERIC_FEATURE_KEYS = census_constants.NUMERIC_FEATURE_KEYS\n",
    "_VOCAB_FEATURE_DICT = census_constants.VOCAB_FEATURE_DICT\n",
    "_BUCKET_FEATURE_DICT = census_constants.BUCKET_FEATURE_DICT\n",
    "_NUM_OOV_BUCKETS = census_constants.NUM_OOV_BUCKETS\n",
    "_LABEL_KEY = census_constants.LABEL_KEY\n",
    "\n",
    "def _gzip_reader_fn(filename):\n",
    "    return tf.data.TFRecordDataset(filenames, compression_typr=\"GZIP\")\n",
    "\n",
    "def _input_fn(file_pattern,\n",
    "            tf_transform_output,\n",
    "            num_epochs=None\n",
    "            batch_size=32) => tf.data.Dataset:\n",
    "    # Get post-transform feature spec\n",
    "    transformed_feature_spec = (\n",
    "        tf_transform_output.transfomred_feature_spec.copy())\n",
    "    # Create baches of data\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_feature_spec,\n",
    "        reader=_gzip_reader_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        label_key=_LABEL_KEY\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        # poop label sice serving inputs do not include the label\n",
    "        feature_spec.pop(_LABEL_KEY)\n",
    "\n",
    "        # Parse raw examples into a dictionary of tensors matching the feature spec\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "         # Transoform the raw examples using the transform graph\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        return serve_tf_examples_fn\n",
    "\n",
    "def _build_keras_model(hidden_units: List[int] = None) -> tf.keras.Model:\n",
    "    model = _wide_and_deep_classifier(\n",
    "        dnn_hidden_units=hidden_units or [100, 70, 50, 25]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def _wide_and_deep_classifier(dnn_hidden_units):\n",
    "    # define input layers for numeric keys\n",
    "    input_numeric = [\n",
    "        tf.keras.layers.Input(name=colname, shape=(1,), dtype=tf.float32)\n",
    "        for colname in _NUMERIC_FEATURE_KEYS\n",
    "    ]\n",
    "    # define input laysers for vocab kays\n",
    "    input_categorical = [\n",
    "        tf.keras.layers.Input(name=colname, shape=(vocab_size + _NUM_OOV_BUCKETS,), dtype=tf.float32)\n",
    "        for colname, vocab_size in _VOCAB_FEATURE_DICT.items()\n",
    "    ]\n",
    "    \n",
    "    # define input layers for bucketkey\n",
    "    input_categorical += [\n",
    "        tf.keras.layers.Input(name=colname, shape=(num_buckets,), dtype=tf.float32)\n",
    "        for colname, num_buckets in _BUCKET_FEATURE_DICT.items()\n",
    "    ]\n",
    "\n",
    "    deep = tf.keras.laysers.concatinate(input_numeric)\n",
    "\n",
    "    for numnodes in dnn_hidden_units:\n",
    "        deep = tf.keras.layers.Dense(numnodes)(deep)\n",
    "    # concatinate categorical inputs\n",
    "    wide = tf.keras.layers.concatinate(input_categorical)\n",
    "    wide = tf.keras.Dense(128, activation='relu')(wide)\n",
    "\n",
    "    combined = tf.keras.layers.concatinate([deep, wide])\n",
    "\n",
    "    output = tf.keras.layers.concatinate([deep, wide])\n",
    "\n",
    "    input_layers = input_numeric + input_categorical\n",
    "    model = tf.keras.Model(input_layers, output)\n",
    "\n",
    "    model.compile(\n",
    "        loss=''\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants\n",
    "ATTR_KEY = \"attributes\"\n",
    "IMAGE_KEY = \"image\"\n",
    "LABEL_KEY = \"Smiling\"\n",
    "GROUP_KEY = \"Young\"\n",
    "IMAGE_SIZE = 28\n",
    "\n",
    "def preprocess_image_dict(feat_dict):\n",
    "    image = feat_dict[IMAGE_KEY]\n",
    "    label = feat_dict[ATTR_KEY][LABEL_KEY]\n",
    "    group = feat_dict[ATTR_KEY][GROUP_KEY]\n",
    "\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image /= 255.0\n",
    "\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    group = tf.cast(group, tf.float32)\n",
    "\n",
    "    feat_dict[IMAGE_KEY] = image\n",
    "    feat_dict[ATTR_KEY][LABEL_KEY] = label\n",
    "    feat_dict[ATTR_KEY][GROUP_KEY] = group\n",
    "\n",
    "    return feat_dict\n",
    "\n",
    "\n",
    "def celeb_a_train_data_wo_group(data, batch_size):\n",
    "    celeb_a_train_data = data.shuffle(1024).repeat().batch(batch_size).map(preprocess_image_dict)\n",
    "    return celeb_a_train_data.map(get_image_label)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
